# Automated censorship through TikTok’s content moderation system: an analysis of word filters in France

# table of contents

## 1 Introduction

<p align="justify"> 
The Chinese government has long kept tight reins on both traditional and new media to avoid potential subversion of its authority. Its tactics often entail strict media controls using monitoring systems and firewalls, shuttering publications and websites, or jailing dissident journalists, bloggers and activists (Xu & Albert, 2017). Even more than simply deleting unfavorable content, the primary domestic political goal is to distract from sensitive debates and hinder collective expression (King et al., 2013). With these efforts, the government tries to prevent a political momentum or potential spill-over effects from international movements abroad (King et al., 2017).
<p align="justify">  
However, in a hyper-connected world, boundaries are transcending. Some social media and communication platforms such as WeChat or TikTok have crossed Chinese borders and managed to penetrate global markets. TikTok is China’s first global software player and a massive social network with more than one billion monthly active users (TikTok, 2021). User numbers on Douyin (抖音), the Chinese version of the app, are expected to exceed 800 million by 2025 (Thomala, 2022). The platform enables users to post videos with visual and musical effects for information or entertainment, and to react with likes or comments. According to TikTok’s community guidelines, all users over 13 years of age, regardless of individual characteristics such as gender or ethnicity, have the right to publish and comment freely on TikTok (TikTok, 2022). The company’s huge success can partially be explained by its recommendation algorithm, which proactively tests its content predictions a user might like (Hern, 2022).
<p align="justify">  
While there is data and reports that demonstrate censorship of media content in China, there is still insufficient academic evidence on whether this censorship affects Chinese platforms when operating abroad (Gray, 2021). Over the past years, TikTok has been highly controversial. For example, India banned TikTok in 2020 due to concerns relating to security of state and public order (Press Information Bureau Delhi, 2020). Similarly, US officials fear that TikTok may be used to conduct large-scale information operations (Thomas, 2022). As this (digital) system competition is gaining momentum, the spotlight turns to TikTok’s opaque content moderation system and its potential as a tool for censorship.
<p align="justify">  
Considering this, TikTok has been subject to various journalistic investigations on content moderation. For example, the Washington Post reported that TikTok's US-based employees had repeatedly been ordered to restrict some videos on its platform at the behest of Beijing-based teams (Harwell & Romm, 2019). Most recently, a German journalist network detected that TikTok presumably uses automated word filters to prevent certain comments from being publicly visible (Eckert et al., 2022).
<p align="justify"> 
Building upon these journalistic investigations, the present study sought to further test TikTok’s automated content moderation of comments in an experimental set-up. In this report, we will first briefly review the existing body of literature on digital content moderation and lay out our methodology. Then, we will describe our results and discuss our findings, before concluding by a discussion of limitations and proposals for future research.
  
## 2 Literature review
<p align="justify"> 
According to Langlois et al. (2009), social media is “primarily concerned with establishing the technocultural conditions within which users can produce content [which] can be re-channelled through techno-commercial networks and channels”. Following this approach, content moderation is an inevitable tool to establish such technocultural conditions on social media platforms (Gillespie, 2020). Building upon Grimmelmann (2015), the present report refers to content moderation as “governance mechanisms that structure participation in a[n online] community to facilitate cooperation and prevent abuse” (cf. Roberts, 2014). With a growing list of potential abuses in the digital world, content moderation systems increasingly rely on “removing or reducing the visibility of potentially abusive content” (Nicholas, 2022). Considering the rapid expansion of social media platforms and the associated immense volume of data, today’s digital environment is dominated by automated content moderation mechanisms via algorithms and artificial intelligence (AI; Gillespie, 2020). 
<p align="justify"> 
Previous literature has studied digital content moderation as a threat to individuals’ freedom of expression (Llansó, 2020), as a challenge to platform governance (Gorwa, Binns & Katzenbach, 2020) or, from a user perspective, as a source of frustration and confusion (Myers West, 2018). Gorwa, Binns & Katzenbach (2020) argue for instance that even if moderation systems are ‘well optimized’, they tend to increase opacity on online platforms and thereby obstruct transparent governance practices. Furthermore, Binns et al. (2017) recognize the inherent risk of bias in AI content moderation systems, as their functioning is conditioned on the norms of those humans who train the data. In line with this, Ozanne et al. (2022) found that users tend to trust less in moderation systems if an AI is involved.
<p align="justify"> 
A contested technique in this respect is shadowbanning, a form of undisclosed content moderation that renders any content, e.g. a comment, invisible to everyone except the original poster (Nicholas, 2022). As such, shadowbanning has an ‘agenda-cutting’ power, which refers to the “process whereby [...] issues have attention directed away from them by receiving little or no media coverage” (Wober & Gunter, 1988). 
<p align="justify"> 
TikTok is a particularly interesting case for shadowbanning as a tool of automated content moderation. This is not only because AI-based content moderation dominates the company’s practices (Grandinetti, 2021), but also because the company has faced criticisms of biased moderation practices in the past (Burke, 2019; Biddle, RIbeiro & Dias, 2020; Lin, 2021). For example, it was found that TikTok had allegedly ordered the censorship of videos that mentioned topics such as Tiananmen Square or Tibetan independence (Hern, 2019) or that political speech in livestreams as well as posts from “undesirable users”, i.e. the ‘unattractive’, ‘poor’, or ‘disabled’, was suppressed (Biddle et al., 2020). While scholars have empirically studied shadowbanning on other social media platforms such as Facebook (Le Merrer, Morgan & Trédan, 2021) or Instagram (Cotter, 2021), they have so far neglected to study the phenomenon on TikTok. Rather, previous studies on TikTok’s content moderation practices have focused on the perceptions of (TikTok) content creators towards algorithmic (in)visibility (Duffy & Meisner, 2022) or theorized TikTok’s ‘visibility moderation’ practices more broadly (Zeng & Kaye, 2022). 
<p align="justify"> 
Recently, a German investigative journalist network discovered that TikTok Germany allegedly restricts users’ freedom of speech by using automated word filters. The experiment showed that at least 20 words (e.g., “LGBTQ“) prevented comments from appearing publicly. According to them, during prior research, words in the context of the Russian war of aggression against Ukraine as well as climate change were also censored occasionally (Eckert et al., 2022). The present report seeks to fill a gap in the literature by replicating the journalistic experiment in the French context, considering that France is the second-biggest market for TikTok in the European Union, with almost 30% of the French population using the app (Kemp, 2022). 

## 3 Methodology
<p align="justify"> 
We designed an experiment in order to analyze if TikTok’s automated content moderation system censors certain word clusters, even if they comply with the platform’s <a href="./product/download.html](https://www.tiktok.com/community-guidelines?lang=en" target="_top">community guidelines</a>. The following section describes the experimental setup and justifies each decision made in order to collect the data in the most rigorous, transparent and systematized way. 

<ins>TikTok accounts</ins>
<p align="justify"> 
The first decision involved the setup of the different accounts that were used in the experiment. We conducted the experiment with eight different accounts, four of them posting word clusters and four of them checking if they appeared in the comment section of the videos. Since only two group members had active accounts, it was decided that one of them would post ‘treatment words’ and the other one ‘control words’. The other six accounts were created for the purpose of this experiment.



</b>

<div align="center">

  **Table 1: Accounts**
  
| **Account type** | **Account history** | **Role** |
|---|---|---|
| Treatment account 1 | Yes | Post treatment words |
| Treatment account 2 | No | Post treatment words |
| Control account 1 | Yes | Post control words |
| Control account 2 | No | Post control words |
| Check account 1 | No | Check if words appear |
| Check account 2 | No | Check if words appear |
| Check account 3 | No | Check if words appear |
| Check account 4 | No | Check if words appear |

  ***Table 1 - Description of accounts used for the experiment.***
  
</div> 
  

<p align="justify"> 
It should be noted that the two accounts that had previous history differ in some minor aspects. While neither had posted videos and had no likes, the account Treatment account 1 had 20 followers and followed 42 other accounts, while the Control account 1 had 91 followers and followed 41 other accounts. Both users reported that they use their accounts frequently but only to watch videos and share them through the app’s in-built function on other messengers and social media platforms. 
  
<ins>Word clusters</ins>
<p align="justify"> 
We decided to test four main word clusters related to the LGBTQ+ community, climate change, the Russian war in Ukraine, and politically sensitive words in China. The first three thematic groups were already tested in the aforementioned German experiment and the fourth cluster was included for this particular experiment. In the remainder of the report, this set of words is referred to as 'treatment words', i.e., terms that could potentially prevent comments from appearing publicly in the comment section of the videos. 
<p align="justify"> 
Following the original setup of the German experiment, in which words were posted in German, for this experiment – carried out in France – all words were translated into French by a fluent-speaking member of the group. In addition, words were embedded in sentences with neutral language that did not violate TikTok’s community guidelines. The same sentences were used for each word cluster and were adjusted to their content in order to make sense and to be grammatically correct. Table 2 shows the words chosen for each word cluster as well as the sentence in which they were embedded. 

 

</b>

<div align="center">  
  
  **Table 2: Treatment words**
  
| **Word cluster** | **Treatment word** | **Sentence** |
|---|---|---|
| LGBT | LGBT | Je soutiens la communauté LGBT |
| LGBT | Homosexuelle | Je soutiens la communauté homosexuelle |
| LGBT | Gay | Je soutiens la communauté gay |
| LGBT | Lesbienne | Je soutiens la communauté lesbienne |
| LGBT | Bisexuelle | Je soutiens la communauté bisexuelle |
| LGBT | Trans | Je soutiens la communauté trans |
| LGBT | Pollution | La pollution est importante |
| Climate Change | Crise climatique | La crise climatique est importante |
| Climate Change | Changement climatique | Le changement climatique est important |
| Climate Change | Émissions | Les émissions sont importantes |
| Climate Change | Protection environnementale | La protection de l'environnement est importante |
| Climate Change | Réchauffement global | Le réchauffement climatique est important |
| Climate Change | Crise énergétique | La crise énergétique est importante |
| Russo-Ukrainian War | Opération spéciale | Nous devons parler d'opérations spéciales en Ukraine |
| Russo-Ukrainian War | Avions de chasse | Nous devons parler des avions de chasse en Ukraine |
| Russo-Ukrainian War | Char de bataille | Nous devons parler des chars de bataille en Ukraine |
| Russo-Ukrainian War | Droit international | Nous devons parler du droit international en Ukraine |
| Russo-Ukrainian War | Troupes | Nous devons parler des troupes en Ukraine |
| Russo-Ukrainian War | Crimes de guerre | Nous devons parler des crimes de guerre en Ukraine |
| Russo-Ukrainian War | Invasion russe | Nous devons parler de l’Invasion russe en Ukraine |
| Russo-Ukrainian War | Boutcha | Nous devons parler de Boutcha en Ukraine |
| China | Ouïghours | Nous devons parler des Ouïghours |
| China | Xinjiang | Nous devons parler de Xinjiang |
| China | Taïwan | Nous devons parler de Taïwan |
| China | Tibet | Nous devons parler du Tibet |
| China | Hongkong | Nous devons parler de Hongkong |
| China | Tiananmen | Nous devons parler de Tiananmen |
| China | Droits de l’homme | Nous devons parler des droits de l’homme |
| China | Travail forcé | Nous devons parler de travail forcé    |
  
  ***Table 2 - Description of treatment words.***
  
</div> 

<p align="justify"> 
In addition, the sentence “J'aime ta vidéo” (I like your video) was used as a control group of words, in order to verify that the comments on each video were not restricted by the owner. This sentence, referred to as ‘control words’ in the remainder of the report, also provides a benchmark to compare the experimental results in relation to the ‘treatment words’.

