# Automated censorship through TikTok’s content moderation system: an analysis of word filters in France

# table of contents

## 1 Introduction

<p align="justify"> 
The Chinese government has long kept tight reins on both traditional and new media to avoid potential subversion of its authority. Its tactics often entail strict media controls using monitoring systems and firewalls, shuttering publications and websites, or jailing dissident journalists, bloggers and activists (Xu & Albert, 2017). Even more than simply deleting unfavorable content, the primary domestic political goal is to distract from sensitive debates and hinder collective expression (King et al., 2013). With these efforts, the government tries to prevent a political momentum or potential spill-over effects from international movements abroad (King et al., 2017). <br>
<p align="justify">  
However, in a hyper-connected world, boundaries are transcending. Some social media and communication platforms such as WeChat or TikTok have crossed Chinese borders and managed to penetrate global markets. TikTok is China’s first global software player and a massive social network with more than one billion monthly active users (TikTok, 2021). User numbers on Douyin (抖音), the Chinese version of the app, are expected to exceed 800 million by 2025 (Thomala, 2022). The platform enables users to post videos with visual and musical effects for information or entertainment, and to react with likes or comments. According to TikTok’s community guidelines, all users over 13 years of age, regardless of individual characteristics such as gender or ethnicity, have the right to publish and comment freely on TikTok (TikTok, 2022). The company’s huge success can partially be explained by its recommendation algorithm, which proactively tests its content predictions a user might like (Hern, 2022). <br>
<p align="justify">  
While there is data and reports that demonstrate censorship of media content in China, there is still insufficient academic evidence on whether this censorship affects Chinese platforms when operating abroad (Gray, 2021). Over the past years, TikTok has been highly controversial. For example, India banned TikTok in 2020 due to concerns relating to security of state and public order (Press Information Bureau Delhi, 2020). Similarly, US officials fear that TikTok may be used to conduct large-scale information operations (Thomas, 2022). As this (digital) system competition is gaining momentum, the spotlight turns to TikTok’s opaque content moderation system and its potential as a tool for censorship. <br>
<p align="justify">  
Considering this, TikTok has been subject to various journalistic investigations on content moderation. For example, the Washington Post reported that TikTok's US-based employees had repeatedly been ordered to restrict some videos on its platform at the behest of Beijing-based teams (Harwell & Romm, 2019). Most recently, a German journalist network detected that TikTok presumably uses automated word filters to prevent certain comments from being publicly visible (Eckert et al., 2022). <br>
<p align="justify"> 
Building upon these journalistic investigations, the present study sought to further test TikTok’s automated content moderation of comments in an experimental set-up. In this report, we will first briefly review the existing body of literature on digital content moderation and lay out our methodology. Then, we will describe our results and discuss our findings, before concluding by a discussion of limitations and proposals for future research. </p>
