# Automated censorship through TikTok’s content moderation system: an analysis of word filters in France

## Table of Contents
[1 Introduction](#introduction)

[2 Literature review](#litreview)

[3 Methodology](#methodology)

[4 Results](#results)

[5 Discussion](#discussion)

[6 Conclusion](#conclusion)

[7 Bibliography](#bibliography)

<a name="introduction"></a>
## 1 Introduction

<p align="justify"> 
The Chinese government has long kept tight reins on both traditional and new media to avoid potential subversion of its authority. Its tactics often entail strict media controls using monitoring systems and firewalls, shuttering publications and websites, or jailing dissident journalists, bloggers and activists (Xu & Albert, 2017). Even more than simply deleting unfavorable content, the primary domestic political goal is to distract from sensitive debates and hinder collective expression (King et al., 2013). With these efforts, the government tries to prevent a political momentum or potential spill-over effects from international movements abroad (King et al., 2017).
<p align="justify">  
However, in a hyper-connected world, boundaries are transcending. Some social media and communication platforms such as WeChat or TikTok have crossed Chinese borders and managed to penetrate global markets. TikTok is China’s first global software player and a massive social network with more than one billion monthly active users (TikTok, 2021). User numbers on Douyin (抖音), the Chinese version of the app, are expected to exceed 800 million by 2025 (Thomala, 2022). The platform enables users to post videos with visual and musical effects for information or entertainment, and to react with likes or comments. According to TikTok’s community guidelines, all users over 13 years of age, regardless of individual characteristics such as gender or ethnicity, have the right to publish and comment freely on TikTok (TikTok, 2022). The company’s huge success can partially be explained by its recommendation algorithm, which proactively tests its content predictions a user might like (Hern, 2022).
<p align="justify">  
While there is data and reports that demonstrate censorship of media content in China, there is still insufficient academic evidence on whether this censorship affects Chinese platforms when operating abroad (Gray, 2021). Over the past years, TikTok has been highly controversial. For example, India banned TikTok in 2020 due to concerns relating to security of state and public order (Press Information Bureau Delhi, 2020). Similarly, US officials fear that TikTok may be used to conduct large-scale information operations (Thomas, 2022). As this (digital) system competition is gaining momentum, the spotlight turns to TikTok’s opaque content moderation system and its potential as a tool for censorship.
<p align="justify">  
Considering this, TikTok has been subject to various journalistic investigations on content moderation. For example, the Washington Post reported that TikTok's US-based employees had repeatedly been ordered to restrict some videos on its platform at the behest of Beijing-based teams (Harwell & Romm, 2019). Most recently, a German journalist network detected that TikTok presumably uses automated word filters to prevent certain comments from being publicly visible (Eckert et al., 2022).
<p align="justify"> 
Building upon these journalistic investigations, the present study sought to further test TikTok’s automated content moderation of comments in an experimental set-up. In this report, we will first briefly review the existing body of literature on digital content moderation and lay out our methodology. Then, we will describe our results and discuss our findings, before concluding by a discussion of limitations and proposals for future research.

  
<a name="litreview"></a>
## 2 Literature review
<p align="justify"> 
According to Langlois et al. (2009), social media is “primarily concerned with establishing the technocultural conditions within which users can produce content [which] can be re-channelled through techno-commercial networks and channels”. Following this approach, content moderation is an inevitable tool to establish such technocultural conditions on social media platforms (Gillespie, 2020). Building upon Grimmelmann (2015), the present report refers to content moderation as “governance mechanisms that structure participation in a[n online] community to facilitate cooperation and prevent abuse” (cf. Roberts, 2014). With a growing list of potential abuses in the digital world, content moderation systems increasingly rely on “removing or reducing the visibility of potentially abusive content” (Nicholas, 2022). Considering the rapid expansion of social media platforms and the associated immense volume of data, today’s digital environment is dominated by automated content moderation mechanisms via algorithms and artificial intelligence (AI; Gillespie, 2020). 
<p align="justify"> 
Previous literature has studied digital content moderation as a threat to individuals’ freedom of expression (Llansó, 2020), as a challenge to platform governance (Gorwa, Binns & Katzenbach, 2020) or, from a user perspective, as a source of frustration and confusion (Myers West, 2018). Gorwa, Binns & Katzenbach (2020) argue for instance that even if moderation systems are ‘well optimized’, they tend to increase opacity on online platforms and thereby obstruct transparent governance practices. Furthermore, Binns et al. (2017) recognize the inherent risk of bias in AI content moderation systems, as their functioning is conditioned on the norms of those humans who train the data. In line with this, Ozanne et al. (2022) found that users tend to trust less in moderation systems if an AI is involved.
<p align="justify"> 
A contested technique in this respect is shadowbanning, a form of undisclosed content moderation that renders any content, e.g. a comment, invisible to everyone except the original poster (Nicholas, 2022). As such, shadowbanning has an ‘agenda-cutting’ power, which refers to the “process whereby [...] issues have attention directed away from them by receiving little or no media coverage” (Wober & Gunter, 1988). 
<p align="justify"> 
TikTok is a particularly interesting case for shadowbanning as a tool of automated content moderation. This is not only because AI-based content moderation dominates the company’s practices (Grandinetti, 2021), but also because the company has faced criticisms of biased moderation practices in the past (Burke, 2019; Biddle, RIbeiro & Dias, 2020; Lin, 2021). For example, it was found that TikTok had allegedly ordered the censorship of videos that mentioned topics such as Tiananmen Square or Tibetan independence (Hern, 2019) or that political speech in livestreams as well as posts from “undesirable users”, i.e. the ‘unattractive’, ‘poor’, or ‘disabled’, was suppressed (Biddle et al., 2020). While scholars have empirically studied shadowbanning on other social media platforms such as Facebook (Le Merrer, Morgan & Trédan, 2021) or Instagram (Cotter, 2021), they have so far neglected to study the phenomenon on TikTok. Rather, previous studies on TikTok’s content moderation practices have focused on the perceptions of (TikTok) content creators towards algorithmic (in)visibility (Duffy & Meisner, 2022) or theorized TikTok’s ‘visibility moderation’ practices more broadly (Zeng & Kaye, 2022). 
<p align="justify"> 
Recently, a German investigative journalist network discovered that TikTok Germany allegedly restricts users’ freedom of speech by using automated word filters. The experiment showed that at least 20 words (e.g., “LGBTQ“) prevented comments from appearing publicly. According to them, during prior research, words in the context of the Russian war of aggression against Ukraine as well as climate change were also censored occasionally (Eckert et al., 2022). The present report seeks to fill a gap in the literature by replicating the journalistic experiment in the French context, considering that France is the second-biggest market for TikTok in the European Union, with almost 30% of the French population using the app (Kemp, 2022). 

<a name="methodology"></a>
## 3 Methodology
<p align="justify"> 
We designed an experiment in order to analyze if TikTok’s automated content moderation system censors certain word clusters, even if they comply with the platform’s <a href="https://www.tiktok.com/community-guidelines?lang=en" target="_top">community guidelines</a>. The following section describes the experimental setup and justifies each decision made in order to collect the data in the most rigorous, transparent and systematized way. 

<ins>TikTok accounts</ins>
<p align="justify"> 
The first decision involved the setup of the different accounts that were used in the experiment. We conducted the experiment with eight different accounts, four of them posting word clusters and four of them checking if they appeared in the comment section of the videos. Since only two group members had active accounts, it was decided that one of them would post ‘treatment words’ and the other one ‘control words’. The other six accounts were created for the purpose of this experiment.



</b>

<div align="center">

  **Table 1: Accounts**
  
| **Account type** | **Account history** | **Role** |
|---|---|---|
| Treatment account 1 | Yes | Post treatment words |
| Treatment account 2 | No | Post treatment words |
| Control account 1 | Yes | Post control words |
| Control account 2 | No | Post control words |
| Check account 1 | No | Check if words appear |
| Check account 2 | No | Check if words appear |
| Check account 3 | No | Check if words appear |
| Check account 4 | No | Check if words appear |

  *Table 1 - Description of accounts used for the experiment.*
  
</div> 
  

<p align="justify"> 
It should be noted that the two accounts that had previous history differ in some minor aspects. While neither had posted videos and had no likes, the account Treatment account 1 had 20 followers and followed 42 other accounts, while the Control account 1 had 91 followers and followed 41 other accounts. Both users reported that they use their accounts frequently but only to watch videos and share them through the app’s in-built function on other messengers and social media platforms. 
  
<ins>Word clusters</ins>
<p align="justify"> 
We decided to test four main word clusters related to the LGBTQ+ community, climate change, the Russian war in Ukraine, and politically sensitive words in China. The first three thematic groups were already tested in the aforementioned German experiment and the fourth cluster was included for this particular experiment. In the remainder of the report, this set of words is referred to as 'treatment words', i.e., terms that could potentially prevent comments from appearing publicly in the comment section of the videos. 
<p align="justify"> 
Following the original setup of the German experiment, in which words were posted in German, for this experiment – carried out in France – all words were translated into French by a fluent-speaking member of the group. In addition, words were embedded in sentences with neutral language that did not violate TikTok’s community guidelines. The same sentences were used for each word cluster and were adjusted to their content in order to make sense and to be grammatically correct. Table 2 shows the words chosen for each word cluster as well as the sentence in which they were embedded. 

 

</b>

<div align="center">  
  
  **Table 2: Treatment words**
  
| **Word cluster** | **Treatment word** | **Sentence** |
|---|---|---|
| LGBT | LGBT | Je soutiens la communauté LGBT |
| LGBT | Homosexuelle | Je soutiens la communauté homosexuelle |
| LGBT | Gay | Je soutiens la communauté gay |
| LGBT | Lesbienne | Je soutiens la communauté lesbienne |
| LGBT | Bisexuelle | Je soutiens la communauté bisexuelle |
| LGBT | Trans | Je soutiens la communauté trans |
| LGBT | Pollution | La pollution est importante |
| Climate Change | Crise climatique | La crise climatique est importante |
| Climate Change | Changement climatique | Le changement climatique est important |
| Climate Change | Émissions | Les émissions sont importantes |
| Climate Change | Protection environnementale | La protection de l'environnement est importante |
| Climate Change | Réchauffement global | Le réchauffement climatique est important |
| Climate Change | Crise énergétique | La crise énergétique est importante |
| Russo-Ukrainian War | Opération spéciale | Nous devons parler d'opérations spéciales en Ukraine |
| Russo-Ukrainian War | Avions de chasse | Nous devons parler des avions de chasse en Ukraine |
| Russo-Ukrainian War | Char de bataille | Nous devons parler des chars de bataille en Ukraine |
| Russo-Ukrainian War | Droit international | Nous devons parler du droit international en Ukraine |
| Russo-Ukrainian War | Troupes | Nous devons parler des troupes en Ukraine |
| Russo-Ukrainian War | Crimes de guerre | Nous devons parler des crimes de guerre en Ukraine |
| Russo-Ukrainian War | Invasion russe | Nous devons parler de l’Invasion russe en Ukraine |
| Russo-Ukrainian War | Boutcha | Nous devons parler de Boutcha en Ukraine |
| China | Ouïghours | Nous devons parler des Ouïghours |
| China | Xinjiang | Nous devons parler de Xinjiang |
| China | Taïwan | Nous devons parler de Taïwan |
| China | Tibet | Nous devons parler du Tibet |
| China | Hongkong | Nous devons parler de Hongkong |
| China | Tiananmen | Nous devons parler de Tiananmen |
| China | Droits de l’homme | Nous devons parler des droits de l’homme |
| China | Travail forcé | Nous devons parler de travail forcé    |
  
  *Table 2 - Description of treatment words.*
  
</div> 

<p align="justify"> 
In addition, the sentence “J'aime ta vidéo” (I like your video) was used as a control group of words, in order to verify that the comments on each video were not restricted by the owner. This sentence, referred to as ‘control words’ in the remainder of the report, also provides a benchmark to compare the experimental results in relation to the ‘treatment words’.
  
<ins>Choice of videos</ins>  
<p align="justify"> 
Another decision that was made was the choice of videos in which the ‘treatment words’ and the ‘control words’ would be posted. The first characteristic of the chosen videos was that they were completely unrelated to the cluster of words being tested. This decision stemmed from the fact that the aim of the research was to test TikTok’s automated moderation system with regards to words in the comment section and not the video content itself. Moreover, the decision aimed to avoid any interaction between the video content and potential comment removal. 
<p align="justify">  
Some videos were selected based on thematic searches, while most of them were taken from the ForYou-Pages of the accounts in order to interact with organic content that TikTok thinks is relevant for the employed accounts. In addition, in order to avoid any kind of interference with the experiment, the video links were not sent through any direct communication between the group members (i.e. a WhatsApp group). Instead, they were written in a Google Drive sheet and the link did not contain the referral part of the URL from which the video was obtained. Figure 1 shows the difference between a link containing a referral identifier and the link for the same video not containing the identifier. </p> 

<div align="center"> 
  
  <img width="452" alt="figure 1" src="https://user-images.githubusercontent.com/115728895/205106867-118ebb85-d673-4a58-b4ce-a0266cf36f77.png">

  
  *Figure 1 - Difference between links with and without referral identifier.*
  </div>

<p align="justify"> 
The second characteristic of the chosen videos is that they had less than 50 comments. This decision had to be made, since comments do not appear chronologically and the checking process of whether the posted comments appeared or not occurred manually. All the characteristics of each video were recorded, including the user name of the video’s owner and the number of followers, the video’s URL, number of views, likes, comments and shares. 

  
<ins>The posting process</ins>
<p align="justify"> 
The posting process was done in two consecutive rounds, the first on November 11th, 2022 and the second on November 20th, 2022. After the first round and in light of the first results, we made the effort to increase the follower number of the first treatment account, in order to make Treatment account 1 and Control account 1 even more equal. 
<p align="justify"> 
Treatment and control words were posted under the same video at approximately the same time. Given that the order in which the words were posted could have had an effect on the results, there was an alternation between which accounts posted first and which ones posted afterwards. Consequently, for some words, the accounts which posted ‘treatment words’ commented first, and for other words the accounts which posted ‘control words’ commented first. The exact times were recorded in a <a href="https://docs.google.com/spreadsheets/d/1rx0xce8TgYR3WbMyKbFbzRDz0wuyHIG6NNcnp1sshjI/edit#gid=0" target="_top">Matrix</a>. 

<ins>The checking process</ins>
<p align="justify"> 
The checking process was done three times by the four checking accounts with the objective of evaluating if there was a change in the visibility of the comments over time. The first check was done right after the words were posted. The second check was carried out one day after posting and the third check was done a week after posting. The checking accounts recorded the results in the <a href="https://docs.google.com/spreadsheets/d/1rx0xce8TgYR3WbMyKbFbzRDz0wuyHIG6NNcnp1sshjI/edit#gid=0" target="_top">Matrix</a>, with 0 indicating that the comment was not visible and 1 indicating that the comment was visible in their respective accounts. 

<a name="results"></a>
## 4 Results
<p align="justify"> 
This section presents our research results in detail. First, the descriptive statistics of the TikTok videos involved in the experiment are reported. In the second part, we present the results obtained from our experiment.
<p align="justify"> 
Our experiment involved 29 conveniently selected videos from TikTok. The average view count amounted to 608,717, the median to 43,300 views. The minimum view count was 359, the maximum was 15,000,000. Since we focused on videos with a manageable number of pre-existing comments, the number of comments ranged between 0 and 64, with an average of 26 and a median of 25. The share count of the videos examined was on average 88 and had a median of 30, ranging between 1 and 388. The average number of likes per video was 9092, ranging from a minimum of 27 up to a maximum of 33,600 likes. This shows that the videos varied in their virality and user engagement on TikTok. 

<div align="center"> 
  
  ![figure 2](https://user-images.githubusercontent.com/115728895/205167800-2d34deab-bf66-4902-b01f-82401f0bea49.svg)
  
  
  *Figure 2 - Visibility of test comments on TikTok (by theme). Note: one box represents one test comment.*
    </div>
    



    
<p align="justify"> 
Figure 2 presents the results of the experiment itself. In the treatment group on the LGBTQ+ topic, no test comment was publicly visible. In contrast, in the control group, 6 of the 12 (50%) tested comments were publicly visible. Likewise, on the topic of Climate Change, no comments were visible in the treatment group, whereas 7 out of 14 (50%) were visible in the control group. In the Russo-Ukrainian War topic, again, no comments within the treatment group were publicly visible. Contrary to that, in the control group, 9 out of 16 (56%) comments were publicly displayed. Lastly, on issues related to China, none of the test comments from the treatment group were visible while 8 out of 16 comments from the control group were not. 
<p align="justify"> 
In total, from the 58 tested comments in the treatment group across topics, not a single comment was found to be publicly visible. In the control group, on the other hand, 30 of the 58 (52%) comments were publicly visible across every theme. Beyond that, in all cases where comments were not publicly visible, they were still displayed as published to the author (see shadowbanning). In addition, the author of the comment was able to interact with the comment (e.g., to like the comment).

<a name="discussion"></a>
## 5 Discussion
<p align="justify"> 
In line with the findings from Germany (see Eckert et al., 2022), our experiment indicates that TikTok is censoring comments without any cause. Yet, our results are not unambiguous. On the one hand, we could observe that all our test words were not publicly visible, although their content was not in violation of TikTok’s community guidelines. On the other hand, a significant number of comments posted in the control group were also not visible. This issue only relates to one of the two test accounts (the one without an account history). 
<p align="justify"> 
This is puzzling to us since both accounts in the control group only posted non-political, affirmative content that fits within what is regularly commented on the platform. The two control group accounts solely differentiate by account history. One has an active and organic user history, while the other (affected) one was created for the purpose of the experiment and, therefore, had no history of prior use. At the same time, one of the treatment group accounts also had an active user history (but no visible comments). 
<p align="justify"> 
Comments were posted under videos that were selected from the ForYou-Page of the treatment accounts, so content that TikTok has suggested and would be organical to interact with, which makes it even more surprising that comments are not shown. This suggests that account history may not be the crucial factor. In addition, the shadowbanning of comments by the affected control account was inconsistent since in one case a comment was visible. In this context, the video involved did not display any characteristics significantly different from the others (i.e., all video characteristics were in between the respective minimum and maximum values). Besides, we did not find that the video characteristics affected the presence of shadowbanning in any other case.
<p align="justify"> 
One might be tempted to argue that some comments were not visible due to seemingly detected spamming behavior. However, there are indications that this does not apply to our experiment. First, our comment behavior was not in violation of TikTok’s policy on spamming (see TikTok, 2022). Second, the number of comments posted per account over a period of two non-consecutive days was not particularly high (29). Third, the issue of shadowbanning was already present from the very first comment. Assuming that TikTok does not classify new accounts as bots by default, this can hardly be explained.
<p align="justify"> 
Apart from that, certain limitations of the study must be recognized. First, the experiment only made use of four posting accounts in total. To establish a clearer picture, future research investigating this issue should aim for creating larger experimental groups. Second, although we have tried to make the test accounts as comparable as possible, it cannot be precluded that latent features or metrics may have had an influence on the presence of shadowbanning. Third, TikTok provides its users with the option of filtering comments by keyword. While we have tried to obviate this by commenting on videos not related to our test topics, we cannot fully rule out the possibility that this feature might have affected our results.
<p align="justify"> 
However, the present research has provided clear evidence of illegitimate shadowbanning practices on TikTok. To what extent these practices depend on the content of comments or on other aspects remains to be explored. Furthermore, it remains essential to demand more transparency from TikTok. As of now, the opacity of the platform’s content moderation algorithm makes it difficult to assess its impact on freedom of speech in Europe and beyond.

<a name="conclusion"></a>
## 6 Conclusion
<p align="justify"> 
In conclusion, triggered by the German precedent, we set up an experimental structure to investigate TikTok’s automated content moderation system in France. For this, four accounts posted treatment or control words embedded in standardized sentences under 29 unrelated videos. Another four accounts then consecutively checked if the comments appeared. In 100% of the treatment and roughly 50% of the control cases, the posted content was not publicly visible in spite of being displayed as published. 
<p align="justify"> 
This experiment has provided evidence of illegitimate shadowbanning practices on TikTok, even though the general functioning of the platform’s content moderation algorithm remains opaque. Additionally, pre-existing differences in the treatment and control accounts or word filtering features for the comment section pose limitations to the results of this research.  
<p align="justify"> 
Thus, further research should target the role account history plays on comment visibility and control for automated word filters. Moreover, more clarification is needed on the platform’s definition and action against spamming behavior. The role of video characteristics (related/unrelated, viral/non viral etc.) could also potentially influence the moderation of politically-sensitive word clusters. Hence, this experiment would benefit from being replicated in other geographical contexts with a larger sample of accounts and videos. Overall, the existing body of literature on digital content moderation might benefit from empirically studying the phenomenon of shadowbanning on TikTok. 
<p align="justify"> 
To sum up, our experiment has shown that TikTok’s content moderation system entails a non-transparent dimension, potentially even a bias against new user accounts or certain words and/or word clusters. As it remains crucial to systematically assess the platform’s impact on political discourse and overall freedom of speech abroad, more transparency from TikTok is needed in order to mitigate risks such as content censorship.

<a name="bibliography"></a>
## 7 Bibliography
Biddle, S., Ribeiro, P. V., & Dias, T. (2020, March 16). *TikTok Told Moderators: Suppress Posts by the “Ugly” and Poor.* The Intercept. https://theintercept.com/2020/03/16/tiktok-app-moderators-users-discrimination/.
  
Binns, R., Veale, M., Kleek, M. V., & Shadbolt, N. (2017). Like trainer, like bot? Inheritance of bias in algorithmic content moderation. In *International conference on social informatics* (pp. 405-415). Springer, Cham.
  
Burke, C. (2019, December 5). *Tiktok admits to hiding content made by fat, LGBTQ, and disabled users.* The Daily Dot. https://www.dailydot.com/irl/tiktok-fat-lgbtq-disabled-creators/ 
  
Cotter, K. (2021). “Shadowbanning is not a thing”: black box gaslighting and the power to independently know and credibly critique algorithms. *Information, Communication & Society*, 1-18.
  
Duffy, B. E., & Meisner, C. (2022). Platform governance at the margins: Social media creators’ experiences with algorithmic (in) visibility. *Media, Culture & Society*. 
  
Eckert, S., Felke, C., & Vitlif, O. (2022, October 5). Wortfilter: TikTok schränkt Meinungsfreiheit ein. *Tagesschau*. https://www.tagesschau.de/investigativ/ndr/tik-tok-begriffe-101.html 
  
Gillespie, T. (2020). Content moderation, AI, and the question of scale. *Big Data & Society*, 7(2).
  
Gorwa, R., Binns, R., & Katzenbach, C. (2020). Algorithmic content moderation: Technical and political challenges in the automation of platform governance. *Big Data & Society*, 7(1).
  
Grandinetti, J. (2021). Examining embedded apparatuses of AI in Facebook and TikTok. *Ai & Society*, 1-14.
  
Gray, J. E. (2021). The geopolitics of ‘platforms’: the TikTok challenge. *Internet Policy Review*, 10(2), 1–26. 
  
Grimmelmann, J. (2015). The virtues of moderation. *Yale JL & Tech.*, 17, 42.
  
Harwell, D., & Romm, T. (2019, November 5). Inside TikTok: A Culture Clash Where U.S. Views about Censorship Often Were Overridden by the Chinese Bosses. *Washington Post*. https://www.washingtonpost.com/technology/2019/11/05/inside-tiktok-culture-clash-where-us-views-about-censorship-often-were-overridden-by-chinese-bosses/.
  
Hern, A. (2019, September 29).Revealed: How TikTok Censors Videos That Do Not Please Beijing. *The Guardian*. https://www.theguardian.com/technology/2019/sep/25/revealed-how-tiktok-censors-videos-that-do-not-please-beijing.
  
Hern, A. (2022, October 25). How TikTok's algorithm made it a success: “It pushes the boundaries” *The Guardian*. https://www.theguardian.com/technology/2022/oct/23/tiktok-rise-algorithm-popularity. 
  
Kemp, S. (2022, April 21). Digital 2022 April Global Statshot Report. DataReportal. https://datareportal.com/reports/digital-2022-april-global-statshot
King, G., Pan, J., & Roberts, M. E. (2013). How censorship in China allows government criticism but silences collective expression. *American political science Review*, 107(2), 326-343.
  
King, G., Pan, J., & Roberts, M. E. (2017). How the Chinese government fabricates social media posts for strategic distraction, not engaged argument. *American political science review*, 111(3), 484-501.
  
Langlois, G., Elmer, G., McKelvey, F., & Devereaux, Z. (2009). Networked publics: The double articulation of code and politics on Facebook. *Canadian Journal of Communication*, 34(3), 415-434.
  
Le Merrer, E., Morgan, B., & Trédan, G. (2021, May). Setting the record straighter on shadow banning. In *IEEE INFOCOM 2021-IEEE Conference on Computer Communications* (pp. 1-10). IEEE.
  
Lin, P. (2021). *TikTok vs Douyin A Security and Privacy Analysis*.
  
Llansó, E. J. (2020). No amount of “AI” in content moderation will solve filtering’s prior-restraint problem. *Big Data & Society*, 7(1).
  
Myers West, S. (2018). Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms. *New Media & Society*, 20(11), 4366-4383.
  
Nicholas, G. (2022). *Shedding Light on Shadowbanning*.
  
Ozanne, M., Bhandari, A., Bazarova, N. N., & DiFranzo, D. (2022). Shall AI moderators be made visible? Perception of accountability and trust in moderation systems on social media platforms. *Big Data & Society*, 9(2).
  
Press Information Bureau Delhi. (2020, June 29). Government Bans 59 mobile apps which are prejudicial to sovereignty and integrity of India, defence of India, security of state and public order. *PIB Delhi*. https://pib.gov.in/PressReleasePage.aspx?PRID=1635206
  
Roberts, S. T. (2014). Behind the screen: The hidden digital labor of commercial content moderation. University of Illinois at Urbana-Champaign.
TikTok (2021, September 27). *Thanks a Billion!* TikTok Newsroom. https://newsroom.tiktok.com/en-us/1-billion-people-on-tiktok.
  
TikTok (2022, October). *Community Guidelines*. TikTok. https://www.tiktok.com/community-guidelines
  
Thomala, L. L. (2022, April 29). *China: Douyin (TikTok) users 2025*. Statista. https://www.statista.com/statistics/1090314/china-douyin-tiktok-user-number/.
  
Thomas, I. (2022, November 18). China and the TikTok threat: How the White House cybersecurity team is thinking about it. *CNBC*. https://www.cnbc.com/2022/11/18/white-house-cyber-director-on-tiktok-chinas-influence-campaign.html
  
Wober, M. & Gunter, B. (1988). *Television and Social Control*. Avebury.
  
Xu, B., & Albert, E. (2017). ‘Media Censorship in China. *Council on Foreign Relations*. https://www.cfr.org/backgrounder/media-censorship-china.
  
Zeng, J., & Kaye, D. B. V. (2022). From content moderation to visibility moderation: A case study of platform governance on TikTok. *Policy & Internet*, 14(1), 79-95.
